{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoseEstimation_TestBed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xw6cTmnrv1D",
        "colab_type": "text"
      },
      "source": [
        "# Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose\n",
        "\n",
        "\n",
        "Pose estimation test bed. I have used the following github repository for reference : https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch\n",
        "\n",
        "Link to the paper : https://arxiv.org/pdf/1811.12004.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Oqx2D6sD-k",
        "colab_type": "text"
      },
      "source": [
        "_____________________________________________________________\n",
        "## **Part 1 :** \n",
        "* Cloning the repository\n",
        "* Downloading the pertrained model mentioned in the repository.\n",
        "* Downloading an image to test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTI_9n9vru02",
        "colab_type": "code",
        "outputId": "53e1228b-a1fb-4976-a35a-7503a0d8e3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lightweight-human-pose-estimation.pytorch'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 92 (delta 3), reused 3 (delta 1), pack-reused 83\u001b[K\n",
            "Unpacking objects: 100% (92/92), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcCWBTczs3TN",
        "colab_type": "code",
        "outputId": "922f3945-6000-4d09-fe39-42f0d2faa94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget \"https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-29 05:28:07--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
            "Resolving download.01.org (download.01.org)... 104.124.235.58, 2600:1417:76:480::4b21, 2600:1417:76:483::4b21\n",
            "Connecting to download.01.org (download.01.org)|104.124.235.58|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87959810 (84M)\n",
            "Saving to: ‘checkpoint_iter_370000.pth’\n",
            "\n",
            "\r          checkpoin   0%[                    ]       0  --.-KB/s               \r         checkpoint  60%[===========>        ]  50.41M   252MB/s               \rcheckpoint_iter_370 100%[===================>]  83.88M   269MB/s    in 0.3s    \n",
            "\n",
            "2020-03-29 05:28:08 (269 MB/s) - ‘checkpoint_iter_370000.pth’ saved [87959810/87959810]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2CyhHna41nh",
        "colab_type": "code",
        "outputId": "96d63fa2-8e59-406d-a821-df71a48cbc20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint_iter_370000.pth  lightweight-human-pose-estimation.pytorch\n",
            "data\t\t\t    output.jpg\n",
            "img1.jpg\t\t    sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUChSNW4tbl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp lightweight-human-pose-estimation.pytorch/demo.py lightweight-human-pose-estimation.pytorch/demo2.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV8H-fPZunYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -o image.jpg \"https://image.shutterstock.com/image-photo/full-body-young-man-standing-260nw-235695058.jpg\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43sGvdS2xhLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYjPIGEbxk3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp data/img1.jpg img1.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLf3TDW1vZjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 lightweight-human-pose-estimation.pytorch/demo2.py --checkpoint-path checkpoint_iter_370000.pth --image data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZev0naX4I9C",
        "colab_type": "text"
      },
      "source": [
        "## **Part 2:**\n",
        "Setting up the test bed \n",
        "* Importing libraries\n",
        "* Adding desired path\n",
        "* Writing desired classes\n",
        "* Modifying the run function as per requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpL4rUWW4mH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import sys\n",
        "sys.path.append('./lightweight-human-pose-estimation.pytorch/')\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "from val import normalize, pad_width"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7nvAZ7m5Caa",
        "colab_type": "text"
      },
      "source": [
        "Writing necessary classes and methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9np-LLQp5CBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image reader class\n",
        "class ImageReader(object):\n",
        "    def __init__(self, file_names):\n",
        "        self.file_names = file_names\n",
        "        self.max_idx = len(file_names)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.idx = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.idx == self.max_idx:\n",
        "            raise StopIteration\n",
        "        img = cv2.imread(self.file_names[self.idx], cv2.IMREAD_COLOR)\n",
        "        if img.size == 0:\n",
        "            raise IOError('Image {} cannot be read'.format(self.file_names[self.idx]))\n",
        "        self.idx = self.idx + 1\n",
        "        return img\n",
        "\n",
        "class VideoReader(object):\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        try:  # OpenCV needs int to read from webcam\n",
        "            self.file_name = int(file_name)\n",
        "        except ValueError:\n",
        "            pass\n",
        "        self.cap = cv2.VideoCapture(self.file_name)\n",
        "        self.h = self.cap.get(3)\n",
        "        self.w = self.cap.get(4)\n",
        "        print(self.h)\n",
        "        print(self.w)\n",
        "\n",
        "    def __iter__(self):\n",
        "        if not self.cap.isOpened():\n",
        "            raise IOError('Video {} cannot be opened'.format(self.file_name))\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        was_read, img = self.cap.read()\n",
        "        if not was_read:\n",
        "            raise StopIteration\n",
        "        return img\n",
        "\n",
        "def infer_fast(net, img, net_input_height_size, stride, upsample_ratio, cpu,\n",
        "               pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n",
        "    height, width, _ = img.shape\n",
        "    scale = net_input_height_size / height\n",
        "\n",
        "    scaled_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
        "    scaled_img = normalize(scaled_img, img_mean, img_scale)\n",
        "    min_dims = [net_input_height_size, max(scaled_img.shape[1], net_input_height_size)]\n",
        "    padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n",
        "\n",
        "    tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float()\n",
        "    if not cpu:\n",
        "        tensor_img = tensor_img.cuda()\n",
        "\n",
        "    stages_output = net(tensor_img)\n",
        "\n",
        "    stage2_heatmaps = stages_output[-2]\n",
        "    heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
        "    heatmaps = cv2.resize(heatmaps, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    stage2_pafs = stages_output[-1]\n",
        "    pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
        "    pafs = cv2.resize(pafs, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    return heatmaps, pafs, scale, pad\n",
        "\n",
        "def run_demo_img(net, img, height_size, cpu, track, smooth):\n",
        "    net = net.eval()\n",
        "    if not cpu:\n",
        "        net = net.cuda()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    # img = cv2.imread(\"./data/img1.jpg\")\n",
        "    \n",
        "    orig_img = img.copy()\n",
        "    heatmaps, pafs, scale, pad = infer_fast(net, img, height_size, stride, upsample_ratio, cpu)\n",
        "\n",
        "    total_keypoints_num = 0\n",
        "    all_keypoints_by_type = []\n",
        "    for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "        total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "    pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, demo=True)\n",
        "    for kpt_id in range(all_keypoints.shape[0]):\n",
        "        all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "        all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "    current_poses = []\n",
        "    for n in range(len(pose_entries)):\n",
        "        if len(pose_entries[n]) == 0:\n",
        "            continue\n",
        "        pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "        for kpt_id in range(num_keypoints):\n",
        "            if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "        pose = Pose(pose_keypoints, pose_entries[n][18])\n",
        "        current_poses.append(pose)\n",
        "\n",
        "    if track:\n",
        "        track_poses(previous_poses, current_poses, smooth=smooth)\n",
        "        previous_poses = current_poses\n",
        "    for pose in current_poses:\n",
        "        pose.draw(img)\n",
        "    img = cv2.addWeighted(orig_img, 0.6, img, 0.4, 0)\n",
        "    for pose in current_poses:\n",
        "        cv2.rectangle(img, (pose.bbox[0], pose.bbox[1]),\n",
        "                        (pose.bbox[0] + pose.bbox[2], pose.bbox[1] + pose.bbox[3]), (0, 255, 0))\n",
        "        if track:\n",
        "            cv2.putText(img, 'id: {}'.format(pose.id), (pose.bbox[0], pose.bbox[1] - 16),\n",
        "                        cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
        "        \n",
        "    # cv2.imwrite(\"output.jpg\",img)\n",
        "    return img\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31x5A35V5bV7",
        "colab_type": "code",
        "outputId": "5ba10154-b923-43a8-d438-c62d20d99a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Main code goes here\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "checkpoint_path = \"checkpoint_iter_370000.pth\"\n",
        "\n",
        "# Creating the netork object\n",
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "\n",
        "input_img_height = 256\n",
        "on_cpu = False\n",
        "track = 1\n",
        "smooth = 1\n",
        "\n",
        "# output = run_demo_img(net, img, input_img_height, on_cpu, track, smooth)\n",
        "\n",
        "video = 1\n",
        "vids = \"./vids/*.mp4\"\n",
        "video_paths = glob.glob(vids)\n",
        "\n",
        "for i, video_path in enumerate(video_paths):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = 30\n",
        "    output_filename = \"./output_vids/output%d.mp4\"%i\n",
        "    w = int(cap.get(3))\n",
        "    h = int(cap.get(4))\n",
        "    fourcc=cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out=cv2.VideoWriter(output_filename,fourcc,fps,(w,h))\n",
        "    num_frames = int(cap.get(7))\n",
        "\n",
        "    for j in tqdm(range(num_frames)):\n",
        "        ret,img = cap.read()\n",
        "        output = run_demo_img(net, img, input_img_height, on_cpu, track, smooth)\n",
        "        out.write(output)\n",
        "\n",
        "    out.release()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 365/365 [00:15<00:00, 23.05it/s]\n",
            "100%|██████████| 816/816 [00:34<00:00, 23.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAl4lYCWO3yU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaxsncZSO9Sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Make sure you allow multiple files to download in your web browser\n",
        "output_folder = \"./output_vids\"\n",
        "\n",
        "files_ = glob.glob(output_folder+\"/*.mp4\")\n",
        "for out_file in files_:\n",
        "    files.download(out_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}